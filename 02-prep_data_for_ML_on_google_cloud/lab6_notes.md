# Lab 6: Dataproc: Qwik Start - Command Line

Same as lab 5, but using command Cloud Shell rather than Console.

## Objectives

- Create a Dataproc cluster using the command line
- Run a simple Apache Spark job
- Modify the number of workers in the cluster

## 1. Create a cluster

From Cloud Shell:

Set the region

```bash
gcloud config set dataproc/region us-west1
```

Save project number and ID in environment variables:

```bash
PROJECT_ID=$(gcloud config get-value project) && \
gcloud config set project $PROJECT_ID

PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format='value(projectNumber)')
```

May need to log in with confirmation code via browser using autogenerated student credentials.

Set storage admin permissions for service account.

```bash
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com \
  --role=roles/storage.admin
```

Enable subnetwork access:

```bash
gcloud compute networks subnets update default --region=us-west1  --enable-private-ip-google-access
```

Create the cluster:

```bash
gcloud dataproc clusters create example-cluster --worker-boot-disk-size 500 --worker-machine-type=e2-standard-4 --master-machine-type=e2-standard-4
```

## 2. Submit a job

```bash
gcloud dataproc jobs submit spark --cluster example-cluster \
  --class org.apache.spark.examples.SparkPi \
  --jars file:///usr/lib/spark/examples/jars/spark-examples.jar -- 1000
```

## 3. Update cluster

```bash
gcloud dataproc clusters update example-cluster --num-workers 4
```

Easy, done!
